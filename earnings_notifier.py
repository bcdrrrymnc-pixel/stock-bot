"""
æ±ºç®—ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ Discordé€šçŸ¥Bot
- TDnetï¼ˆæ±è¨¼é©æ™‚é–‹ç¤ºï¼‰ã§æ±ºç®—çŸ­ä¿¡ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾— â† ãƒ¡ã‚¤ãƒ³
- EDINET APIã§æ¥­ç¸¾ä¿®æ­£ãƒ»è–¬äº‹æ‰¿èªãªã©ã‚’è£œå®Œ
- yfinanceã§è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import os
import json
import time
import requests
import yfinance as yf
from bs4 import BeautifulSoup
from collections import Counter
from datetime import datetime, date, timedelta
from pathlib import Path

DISCORD_EARNINGS_WEBHOOK = os.environ["DISCORD_EARNINGS_WEBHOOK"]
DISCORD_NEWS_WEBHOOK     = os.environ["DISCORD_NEWS_WEBHOOK"]
EDINET_API_KEY           = os.environ.get("EDINET_API_KEY", "")

SENT_FILE    = Path("sent_ids.json")
EDINET_BASE  = "https://api.edinet-fsa.go.jp/api/v2"
TDNET_URL    = "https://www.release.tdnet.info/inbs/I_main_00.html"

EDINET_SKIP = [
    "æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸", "å››åŠæœŸå ±å‘Šæ›¸", "åŠæœŸå ±å‘Šæ›¸",
    "è‡¨æ™‚å ±å‘Šæ›¸", "å†…éƒ¨çµ±åˆ¶å ±å‘Šæ›¸", "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸",
    "å¤‰æ›´å ±å‘Šæ›¸", "å…¬é–‹è²·ä»˜", "è¨‚æ­£", "æœ‰ä¾¡è¨¼åˆ¸å±Šå‡ºæ›¸",
]

def load_sent() -> set:
    if SENT_FILE.exists():
        data = json.loads(SENT_FILE.read_text(encoding="utf-8"))
        return set(data.get("ids", []))
    return set()

def save_sent(sent: set):
    ids = list(sent)[-3000:]
    SENT_FILE.write_text(json.dumps({"ids": ids}, ensure_ascii=False, indent=2), encoding="utf-8")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TDnet ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_tdnet_disclosures() -> list[dict]:
    results = []
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        r = requests.get(TDNET_URL, headers=headers, timeout=30)
        r.encoding = "utf-8"
        soup = BeautifulSoup(r.text, "html.parser")

        # â”€â”€ ãƒ‡ãƒãƒƒã‚°ï¼šHTMLæ§‹é€ ã‚’ç¢ºèª â”€â”€
        print(f"[TDnet] HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {r.status_code}")
        print(f"[TDnet] ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {soup.title}")

        # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
        tables = soup.find_all("table")
        print(f"[TDnet] ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
        for t in tables[:5]:
            print(f"  table id={t.get('id')!r} class={t.get('class')!r}")

        # divã®idä¸€è¦§ï¼ˆæ§‹é€ æŠŠæ¡ï¼‰
        divs = soup.find_all("div", id=True)
        print(f"[TDnet] div idä¸€è¦§: {[d.get('id') for d in divs[:20]]}")

        # trã‚’å…¨éƒ¨è©¦ã™
        all_rows = soup.find_all("tr")
        print(f"[TDnet] trç·æ•°: {len(all_rows)}")
        for row in all_rows[:5]:
            cols = row.find_all("td")
            if cols:
                print(f"  tdæ•°={len(cols)} | {[c.get_text(strip=True)[:30] for c in cols[:4]]}")

        # â”€â”€ ãƒ‘ãƒ¼ã‚¹è©¦è¡Œ1: id="main-list-table" â”€â”€
        tbl = soup.select_one("table#main-list-table")
        if tbl:
            rows = tbl.find_all("tr")
            print(f"[TDnet] main-list-table rows: {len(rows)}")
        else:
            print("[TDnet] main-list-table ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨trã§è©¦ã¿ã¾ã™ã€‚")
            rows = all_rows

        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 4:
                continue
            time_str = cols[0].get_text(strip=True)
            ticker   = cols[1].get_text(strip=True)
            company  = cols[2].get_text(strip=True)
            title_td = cols[3]
            title    = title_td.get_text(strip=True)
            a_tag    = title_td.find("a")
            href     = ""
            if a_tag and a_tag.get("href"):
                base = "https://www.release.tdnet.info/inbs/"
                href = base + a_tag["href"].lstrip("./")
            doc_id = href.split("=")[-1] if "=" in href else f"{ticker}_{title[:20]}"

            results.append({
                "id": doc_id, "company": company, "ticker": ticker,
                "title": title, "time": time_str, "url": href, "source": "tdnet",
            })

        print(f"[TDnet] ãƒ‘ãƒ¼ã‚¹çµæœ: {len(results)}ä»¶")
        if results:
            print(f"[TDnet] ã‚µãƒ³ãƒ—ãƒ«: {results[0]}")

    except Exception as e:
        print(f"[TDnet] ã‚¨ãƒ©ãƒ¼: {e}")

    return results

def classify_tdnet(item: dict) -> str | None:
    title = item.get("title", "")
    if any(kw in title for kw in ["æ±ºç®—çŸ­ä¿¡", "å››åŠæœŸæ±ºç®—çŸ­ä¿¡", "ä¸­é–“æ±ºç®—çŸ­ä¿¡"]):
        return "earnings"
    if any(kw in title for kw in ["ä¸Šæ–¹ä¿®æ­£", "ä¸‹æ–¹ä¿®æ­£", "æ¥­ç¸¾ä¿®æ­£", "æ¥­ç¸¾äºˆæƒ³ã®ä¿®æ­£"]):
        return "revision"
    if any(kw in title for kw in ["è–¬äº‹", "FDA", "æ²»é¨“", "æ–°è–¬", "æ‰¿èªå–å¾—", "è£½é€ è²©å£²æ‰¿èª"]):
        return "pharma"
    return None

def edinet_headers() -> dict:
    return {"Ocp-Apim-Subscription-Key": EDINET_API_KEY} if EDINET_API_KEY else {}

def fetch_edinet_documents(target_date: str) -> list[dict]:
    url = f"{EDINET_BASE}/documents.json"
    params = {"date": target_date, "type": 2 if EDINET_API_KEY else 1}
    try:
        r = requests.get(url, params=params, headers=edinet_headers(), timeout=30)
        r.raise_for_status()
        results = r.json().get("results", [])
        print(f"[EDINET] {target_date} â†’ {len(results)}ä»¶")
        return results
    except Exception as e:
        print(f"[EDINET] ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def classify_edinet(doc: dict) -> str | None:
    desc = doc.get("docDescription", "")
    if any(kw in desc for kw in EDINET_SKIP):
        return None
    if any(kw in desc for kw in ["ä¸Šæ–¹ä¿®æ­£", "ä¸‹æ–¹ä¿®æ­£", "æ¥­ç¸¾ä¿®æ­£", "æ¥­ç¸¾äºˆæƒ³ã®ä¿®æ­£"]):
        return "revision"
    if any(kw in desc for kw in ["è–¬äº‹", "FDA", "æ²»é¨“", "æ–°è–¬", "æ‰¿èªå–å¾—", "è£½é€ è²©å£²æ‰¿èª"]):
        return "pharma"
    return None

def get_financials(ticker_jp: str) -> dict:
    if not ticker_jp or not ticker_jp.isdigit():
        return {}
    symbol = f"{ticker_jp}.T"
    try:
        tk = yf.Ticker(symbol)
        info = tk.info
        fin  = tk.financials
        revenue = net_income = None
        if not fin.empty:
            rev_key = [k for k in fin.index if "Revenue" in k]
            inc_key = [k for k in fin.index if "Net Income" in k]
            if rev_key: revenue    = fin.loc[rev_key[0]].iloc[0]
            if inc_key: net_income = fin.loc[inc_key[0]].iloc[0]
        return {
            "company":    info.get("longName") or info.get("shortName", symbol),
            "sector":     info.get("sector", ""),
            "revenue":    revenue,
            "net_income": net_income,
            "total_debt": info.get("totalDebt"),
        }
    except Exception as e:
        print(f"[yfinance] {ticker_jp} ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def fmt_yen(value) -> str:
    if value is None: return "N/A"
    v = float(value)
    if abs(v) >= 1e12: return f"{v/1e12:.2f}å…†å††"
    if abs(v) >= 1e8:  return f"{v/1e8:.1f}å„„å††"
    return f"{v/1e4:.0f}ä¸‡å††"

def build_earnings_embed(item: dict, fin: dict) -> dict:
    ticker  = item.get("ticker", "").strip()
    company = fin.get("company") or item.get("company", "ä¸æ˜")
    sector  = fin.get("sector") or "ä¸æ˜"
    title   = item.get("title", "")
    doc_url = item.get("url", "https://www.release.tdnet.info")
    heading = f"ğŸ“Š {company}" + (f"ï¼ˆ{ticker}ï¼‰" if ticker else "") + " æ±ºç®—ç™ºè¡¨"
    return {
        "username": "æ±ºç®—Bot",
        "embeds": [{
            "title": heading, "description": title, "url": doc_url, "color": 0x00b4d8,
            "fields": [
                {"name": "ğŸ’¹ å£²ä¸Šé«˜",     "value": fmt_yen(fin.get("revenue")),    "inline": True},
                {"name": "ğŸ“ˆ ç´”åˆ©ç›Š",     "value": fmt_yen(fin.get("net_income")), "inline": True},
                {"name": "ğŸ¦ æœ‰åˆ©å­è² å‚µ", "value": fmt_yen(fin.get("total_debt")), "inline": True},
            ],
            "footer": {"text": f"ã‚»ã‚¯ã‚¿ãƒ¼: {sector} | {item.get('time','')} | TDnet"},
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }]
    }

def build_news_embed(company, ticker, title, url, doc_type, source="TDnet") -> dict:
    type_map = {
        "revision": ("ğŸ”„ æ¥­ç¸¾ä¿®æ­£", 0xe63946 if "ä¸‹æ–¹" in title else 0x2dc653),
        "pharma":   ("ğŸ’Š æ–°è–¬ãƒ»è–¬äº‹æ‰¿èª", 0x9b5de5),
    }
    label, color = type_map.get(doc_type, ("ğŸ“Œ é–‹ç¤ºæƒ…å ±", 0xadb5bd))
    heading = f"{label}ï½œ{company}" + (f"ï¼ˆ{ticker}ï¼‰" if ticker else "")
    return {
        "username": "ãƒ‹ãƒ¥ãƒ¼ã‚¹Bot",
        "embeds": [{"title": heading, "description": title[:200], "url": url,
                    "color": color, "footer": {"text": source},
                    "timestamp": datetime.utcnow().isoformat() + "Z"}]
    }

def post_discord(webhook_url: str, payload: dict):
    if not webhook_url:
        print("[Discord] Webhook URLãŒç©ºã§ã™ã€‚")
        return
    r = requests.post(webhook_url, json=payload, timeout=15)
    if r.status_code == 429:
        retry = int(r.headers.get("Retry-After", 5))
        time.sleep(retry)
        requests.post(webhook_url, json=payload, timeout=15)
    elif r.status_code not in (200, 204):
        print(f"[Discord] ã‚¨ãƒ©ãƒ¼ {r.status_code}: {r.text[:200]}")
    else:
        print("[Discord] é€ä¿¡æˆåŠŸ")

def main():
    sent = load_sent()
    new_sent = 0
    print(f"[é€ä¿¡æ¸ˆã¿ID] {len(sent)}ä»¶ã‚’ãƒ­ãƒ¼ãƒ‰")

    # TDnet
    tdnet_items = fetch_tdnet_disclosures()
    for item in tdnet_items:
        itype = classify_tdnet(item)
        if not itype: continue
        doc_id = f"tdnet_{item['id']}"
        if doc_id in sent: continue
        ticker = item.get("ticker", "").strip()
        if itype == "earnings":
            fin = get_financials(ticker) if ticker else {}
            post_discord(DISCORD_EARNINGS_WEBHOOK, build_earnings_embed(item, fin))
            print(f"[æ±ºç®—é€ä¿¡] {item['company']}ï¼ˆ{ticker}ï¼‰")
        else:
            post_discord(DISCORD_NEWS_WEBHOOK, build_news_embed(
                item["company"], ticker, item["title"], item["url"], itype))
            print(f"[ãƒ‹ãƒ¥ãƒ¼ã‚¹é€ä¿¡] {itype} / {item['company']}")
        sent.add(doc_id)
        new_sent += 1
        time.sleep(1)

    # EDINETè£œå®Œ
    edinet_docs = []
    for days_ago in range(3):
        target = (date.today() - timedelta(days=days_ago)).strftime("%Y-%m-%d")
        edinet_docs = fetch_edinet_documents(target)
        if edinet_docs: break

    for doc in edinet_docs:
        doc_id = f"edinet_{doc.get('docID','')}"
        if doc_id in sent: continue
        dtype = classify_edinet(doc)
        if not dtype: continue
        ticker = (doc.get("secCode") or "").strip()
        desc   = doc.get("docDescription", "")
        url    = f"https://disclosure2.edinet-fsa.go.jp/WZEK0040.aspx?S1{doc.get('docID','')}"
        post_discord(DISCORD_NEWS_WEBHOOK, build_news_embed(
            doc.get("filerName","ä¸æ˜"), ticker, desc, url, dtype, "EDINET"))
        print(f"[ãƒ‹ãƒ¥ãƒ¼ã‚¹é€ä¿¡EDINET] {dtype} / {doc.get('filerName')}")
        sent.add(doc_id)
        new_sent += 1
        time.sleep(1)

    save_sent(sent)
    print(f"å®Œäº†ã€‚æ–°è¦é€ä¿¡: {new_sent}ä»¶")

if __name__ == "__main__":
    main()
